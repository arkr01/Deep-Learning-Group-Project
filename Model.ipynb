{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([26786, 165]) torch.Size([26786, 165]) torch.Size([26786, 13])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lachl\\anaconda3\\envs\\ml\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3418: DtypeWarning: Columns (14) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "C:\\Users\\lachl\\Google Drive\\Uni Year 5\\git_stuff\\STAT3007Project\\embedding_players.py:47: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  ret_val[wicket_title] = 1 * (ret_val[\"wicket_kind\"] == wicket_title)\n"
     ]
    }
   ],
   "source": [
    "from embedding_players import get_embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU not available, CPU used\n"
     ]
    }
   ],
   "source": [
    "# torch.cuda.is_available() checks and returns a Boolean True if a GPU is available, else it'll return False\n",
    "is_cuda = torch.cuda.is_available()\n",
    "\n",
    "# If we have a GPU available, we'll set our device to GPU. We'll use this device variable later in our code.\n",
    "if is_cuda:\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"GPU is available\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"GPU not available, CPU used\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filepaths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "BBB_filepath = \"C:/Users/lachl/Google Drive/Uni Year 5/git_stuff/STAT3007Project/csv_files/cricsheet_data_partial.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: train loss: 0.7310588359832764\n",
      "Epoch 1: train loss: 0.7261393070220947\n",
      "Epoch 2: train loss: 0.7212622761726379\n",
      "Epoch 3: train loss: 0.7164236307144165\n",
      "Epoch 4: train loss: 0.7116196155548096\n",
      "Epoch 5: train loss: 0.7068466544151306\n",
      "Epoch 6: train loss: 0.7021014094352722\n",
      "Epoch 7: train loss: 0.6973804235458374\n",
      "Epoch 8: train loss: 0.6926807761192322\n",
      "Epoch 9: train loss: 0.6879991888999939\n",
      "Epoch 10: train loss: 0.6833328604698181\n",
      "Epoch 11: train loss: 0.6786790490150452\n",
      "Epoch 12: train loss: 0.6740350127220154\n",
      "Epoch 13: train loss: 0.6693981289863586\n",
      "Epoch 14: train loss: 0.6647658348083496\n",
      "Epoch 15: train loss: 0.6601359248161316\n",
      "Epoch 16: train loss: 0.6555059552192688\n",
      "Epoch 17: train loss: 0.650873601436615\n",
      "Epoch 18: train loss: 0.646236777305603\n",
      "Epoch 19: train loss: 0.6415933966636658\n",
      "Epoch 20: train loss: 0.6369416117668152\n",
      "Epoch 21: train loss: 0.6322792172431946\n",
      "Epoch 22: train loss: 0.627604603767395\n",
      "Epoch 23: train loss: 0.6229159235954285\n",
      "Epoch 24: train loss: 0.6182114481925964\n",
      "Epoch 25: train loss: 0.6134896874427795\n",
      "Epoch 26: train loss: 0.6087491512298584\n",
      "Epoch 27: train loss: 0.6039883494377136\n",
      "Epoch 28: train loss: 0.5992060899734497\n",
      "Epoch 29: train loss: 0.5944010019302368\n",
      "Epoch 30: train loss: 0.5895720720291138\n",
      "Epoch 31: train loss: 0.5847182273864746\n",
      "Epoch 32: train loss: 0.5798386335372925\n",
      "Epoch 33: train loss: 0.5749324560165405\n",
      "Epoch 34: train loss: 0.5699989795684814\n",
      "Epoch 35: train loss: 0.5650376677513123\n",
      "Epoch 36: train loss: 0.5600481629371643\n",
      "Epoch 37: train loss: 0.555030107498169\n",
      "Epoch 38: train loss: 0.5499833822250366\n",
      "Epoch 39: train loss: 0.5449078679084778\n",
      "Epoch 40: train loss: 0.5398038029670715\n",
      "Epoch 41: train loss: 0.5346713662147522\n",
      "Epoch 42: train loss: 0.5295109748840332\n",
      "Epoch 43: train loss: 0.5243232846260071\n",
      "Epoch 44: train loss: 0.5191090106964111\n",
      "Epoch 45: train loss: 0.5138689279556274\n",
      "Epoch 46: train loss: 0.5086041688919067\n",
      "Epoch 47: train loss: 0.5033159255981445\n",
      "Epoch 48: train loss: 0.4980055093765259\n",
      "Epoch 49: train loss: 0.49267449975013733\n",
      "Epoch 50: train loss: 0.4873245656490326\n",
      "Epoch 51: train loss: 0.4819575250148773\n",
      "Epoch 52: train loss: 0.4765753746032715\n",
      "Epoch 53: train loss: 0.4711802303791046\n",
      "Epoch 54: train loss: 0.46577441692352295\n",
      "Epoch 55: train loss: 0.4603603184223175\n",
      "Epoch 56: train loss: 0.45494040846824646\n",
      "Epoch 57: train loss: 0.4495173394680023\n",
      "Epoch 58: train loss: 0.4440940320491791\n",
      "Epoch 59: train loss: 0.4386732876300812\n",
      "Epoch 60: train loss: 0.4332579970359802\n",
      "Epoch 61: train loss: 0.42785128951072693\n",
      "Epoch 62: train loss: 0.42245617508888245\n",
      "Epoch 63: train loss: 0.4170760214328766\n",
      "Epoch 64: train loss: 0.4117138087749481\n",
      "Epoch 65: train loss: 0.40637293457984924\n",
      "Epoch 66: train loss: 0.40105655789375305\n",
      "Epoch 67: train loss: 0.3957679569721222\n",
      "Epoch 68: train loss: 0.3905104994773865\n",
      "Epoch 69: train loss: 0.38528725504875183\n",
      "Epoch 70: train loss: 0.38010138273239136\n",
      "Epoch 71: train loss: 0.3749561607837677\n",
      "Epoch 72: train loss: 0.36985450983047485\n",
      "Epoch 73: train loss: 0.36479952931404114\n",
      "Epoch 74: train loss: 0.359794020652771\n",
      "Epoch 75: train loss: 0.3548407256603241\n",
      "Epoch 76: train loss: 0.34994247555732727\n",
      "Epoch 77: train loss: 0.3451017141342163\n",
      "Epoch 78: train loss: 0.340320885181427\n",
      "Epoch 79: train loss: 0.3356023132801056\n",
      "Epoch 80: train loss: 0.3309481143951416\n",
      "Epoch 81: train loss: 0.3263602554798126\n",
      "Epoch 82: train loss: 0.32184064388275146\n",
      "Epoch 83: train loss: 0.31739088892936707\n",
      "Epoch 84: train loss: 0.31301257014274597\n",
      "Epoch 85: train loss: 0.3087070882320404\n",
      "Epoch 86: train loss: 0.3044755458831787\n",
      "Epoch 87: train loss: 0.30031904578208923\n",
      "Epoch 88: train loss: 0.296238511800766\n",
      "Epoch 89: train loss: 0.2922345995903015\n",
      "Epoch 90: train loss: 0.28830790519714355\n",
      "Epoch 91: train loss: 0.28445887565612793\n",
      "Epoch 92: train loss: 0.28068774938583374\n",
      "Epoch 93: train loss: 0.2769947350025177\n",
      "Epoch 94: train loss: 0.2733798325061798\n",
      "Epoch 95: train loss: 0.26984283328056335\n",
      "Epoch 96: train loss: 0.2663835883140564\n",
      "Epoch 97: train loss: 0.2630017101764679\n",
      "Epoch 98: train loss: 0.25969672203063965\n",
      "Epoch 99: train loss: 0.2564680576324463\n",
      "Epoch 100: train loss: 0.2533150613307953\n",
      "Epoch 101: train loss: 0.25023695826530457\n",
      "Epoch 102: train loss: 0.24723294377326965\n",
      "Epoch 103: train loss: 0.24430210888385773\n",
      "Epoch 104: train loss: 0.2414434254169464\n",
      "Epoch 105: train loss: 0.2386559098958969\n",
      "Epoch 106: train loss: 0.2359384298324585\n",
      "Epoch 107: train loss: 0.23328988254070282\n",
      "Epoch 108: train loss: 0.23070906102657318\n",
      "Epoch 109: train loss: 0.22819474339485168\n",
      "Epoch 110: train loss: 0.22574572265148163\n",
      "Epoch 111: train loss: 0.22336065769195557\n",
      "Epoch 112: train loss: 0.22103828191757202\n",
      "Epoch 113: train loss: 0.21877725422382355\n",
      "Epoch 114: train loss: 0.21657627820968628\n",
      "Epoch 115: train loss: 0.21443401277065277\n",
      "Epoch 116: train loss: 0.21234910190105438\n",
      "Epoch 117: train loss: 0.21032020449638367\n",
      "Epoch 118: train loss: 0.20834596455097198\n",
      "Epoch 119: train loss: 0.20642508566379547\n",
      "Epoch 120: train loss: 0.20455621182918549\n",
      "Epoch 121: train loss: 0.20273803174495697\n",
      "Epoch 122: train loss: 0.20096924901008606\n",
      "Epoch 123: train loss: 0.1992485672235489\n",
      "Epoch 124: train loss: 0.1975747048854828\n",
      "Epoch 125: train loss: 0.19594644010066986\n",
      "Epoch 126: train loss: 0.19436249136924744\n",
      "Epoch 127: train loss: 0.1928216516971588\n",
      "Epoch 128: train loss: 0.19132272899150848\n",
      "Epoch 129: train loss: 0.18986456096172333\n",
      "Epoch 130: train loss: 0.1884460151195526\n",
      "Epoch 131: train loss: 0.18706591427326202\n",
      "Epoch 132: train loss: 0.1857231855392456\n",
      "Epoch 133: train loss: 0.1844167709350586\n",
      "Epoch 134: train loss: 0.18314555287361145\n",
      "Epoch 135: train loss: 0.18190859258174896\n",
      "Epoch 136: train loss: 0.18070481717586517\n",
      "Epoch 137: train loss: 0.17953327298164368\n",
      "Epoch 138: train loss: 0.17839300632476807\n",
      "Epoch 139: train loss: 0.17728309333324432\n",
      "Epoch 140: train loss: 0.17620263993740082\n",
      "Epoch 141: train loss: 0.17515073716640472\n",
      "Epoch 142: train loss: 0.17412656545639038\n",
      "Epoch 143: train loss: 0.17312927544116974\n",
      "Epoch 144: train loss: 0.17215807735919952\n",
      "Epoch 145: train loss: 0.17121213674545288\n",
      "Epoch 146: train loss: 0.1702907532453537\n",
      "Epoch 147: train loss: 0.1693931668996811\n",
      "Epoch 148: train loss: 0.16851866245269775\n",
      "Epoch 149: train loss: 0.16766655445098877\n",
      "Epoch 150: train loss: 0.16683614253997803\n",
      "Epoch 151: train loss: 0.1660267859697342\n",
      "Epoch 152: train loss: 0.16523785889148712\n",
      "Epoch 153: train loss: 0.16446876525878906\n",
      "Epoch 154: train loss: 0.16371887922286987\n",
      "Epoch 155: train loss: 0.1629876345396042\n",
      "Epoch 156: train loss: 0.16227446496486664\n",
      "Epoch 157: train loss: 0.16157884895801544\n",
      "Epoch 158: train loss: 0.1609002649784088\n",
      "Epoch 159: train loss: 0.16023816168308258\n",
      "Epoch 160: train loss: 0.15959212183952332\n",
      "Epoch 161: train loss: 0.15896162390708923\n",
      "Epoch 162: train loss: 0.15834619104862213\n",
      "Epoch 163: train loss: 0.15774542093276978\n",
      "Epoch 164: train loss: 0.15715888142585754\n",
      "Epoch 165: train loss: 0.15658612549304962\n",
      "Epoch 166: train loss: 0.15602678060531616\n",
      "Epoch 167: train loss: 0.15548042953014374\n",
      "Epoch 168: train loss: 0.1549467146396637\n",
      "Epoch 169: train loss: 0.1544252634048462\n",
      "Epoch 170: train loss: 0.15391570329666138\n",
      "Epoch 171: train loss: 0.15341773629188538\n",
      "Epoch 172: train loss: 0.15293100476264954\n",
      "Epoch 173: train loss: 0.1524551659822464\n",
      "Epoch 174: train loss: 0.15198996663093567\n",
      "Epoch 175: train loss: 0.1515350639820099\n",
      "Epoch 176: train loss: 0.15109016001224518\n",
      "Epoch 177: train loss: 0.15065501630306244\n",
      "Epoch 178: train loss: 0.1502293199300766\n",
      "Epoch 179: train loss: 0.14981283247470856\n",
      "Epoch 180: train loss: 0.14940530061721802\n",
      "Epoch 181: train loss: 0.14900647103786469\n",
      "Epoch 182: train loss: 0.14861609041690826\n",
      "Epoch 183: train loss: 0.14823395013809204\n",
      "Epoch 184: train loss: 0.14785979688167572\n",
      "Epoch 185: train loss: 0.14749348163604736\n",
      "Epoch 186: train loss: 0.1471346914768219\n",
      "Epoch 187: train loss: 0.14678329229354858\n",
      "Epoch 188: train loss: 0.1464391052722931\n",
      "Epoch 189: train loss: 0.14610187709331512\n",
      "Epoch 190: train loss: 0.14577145874500275\n",
      "Epoch 191: train loss: 0.14544768631458282\n",
      "Epoch 192: train loss: 0.14513035118579865\n",
      "Epoch 193: train loss: 0.14481930434703827\n",
      "Epoch 194: train loss: 0.14451438188552856\n",
      "Epoch 195: train loss: 0.1442154198884964\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 196: train loss: 0.14392228424549103\n",
      "Epoch 197: train loss: 0.14363478124141693\n",
      "Epoch 198: train loss: 0.14335282146930695\n",
      "Epoch 199: train loss: 0.14307624101638794\n",
      "Epoch 200: train loss: 0.14280492067337036\n",
      "Epoch 201: train loss: 0.14253869652748108\n",
      "Epoch 202: train loss: 0.14227744936943054\n",
      "Epoch 203: train loss: 0.1420210748910904\n",
      "Epoch 204: train loss: 0.14176946878433228\n",
      "Epoch 205: train loss: 0.14152248203754425\n",
      "Epoch 206: train loss: 0.14127998054027557\n",
      "Epoch 207: train loss: 0.14104193449020386\n",
      "Epoch 208: train loss: 0.1408081352710724\n",
      "Epoch 209: train loss: 0.14057858288288116\n",
      "Epoch 210: train loss: 0.14035311341285706\n",
      "Epoch 211: train loss: 0.1401316374540329\n",
      "Epoch 212: train loss: 0.13991406559944153\n",
      "Epoch 213: train loss: 0.13970030844211578\n",
      "Epoch 214: train loss: 0.1394902914762497\n",
      "Epoch 215: train loss: 0.1392839103937149\n",
      "Epoch 216: train loss: 0.13908106088638306\n",
      "Epoch 217: train loss: 0.13888171315193176\n",
      "Epoch 218: train loss: 0.13868574798107147\n",
      "Epoch 219: train loss: 0.1384931057691574\n",
      "Epoch 220: train loss: 0.1383036971092224\n",
      "Epoch 221: train loss: 0.1381174623966217\n",
      "Epoch 222: train loss: 0.1379343420267105\n",
      "Epoch 223: train loss: 0.13775424659252167\n",
      "Epoch 224: train loss: 0.1375771015882492\n",
      "Epoch 225: train loss: 0.13740286231040955\n",
      "Epoch 226: train loss: 0.13723143935203552\n",
      "Epoch 227: train loss: 0.13706280291080475\n",
      "Epoch 228: train loss: 0.13689689338207245\n",
      "Epoch 229: train loss: 0.13673360645771027\n",
      "Epoch 230: train loss: 0.1365729421377182\n",
      "Epoch 231: train loss: 0.1364148110151291\n",
      "Epoch 232: train loss: 0.13625916838645935\n",
      "Epoch 233: train loss: 0.1361059695482254\n",
      "Epoch 234: train loss: 0.13595513999462128\n",
      "Epoch 235: train loss: 0.1358066350221634\n",
      "Epoch 236: train loss: 0.13566043972969055\n",
      "Epoch 237: train loss: 0.13551649451255798\n",
      "Epoch 238: train loss: 0.13537470996379852\n",
      "Epoch 239: train loss: 0.13523508608341217\n",
      "Epoch 240: train loss: 0.13509757816791534\n",
      "Epoch 241: train loss: 0.13496211171150208\n",
      "Epoch 242: train loss: 0.13482865691184998\n",
      "Epoch 243: train loss: 0.13469721376895905\n",
      "Epoch 244: train loss: 0.13456767797470093\n",
      "Epoch 245: train loss: 0.13444006443023682\n",
      "Epoch 246: train loss: 0.13431429862976074\n",
      "Epoch 247: train loss: 0.1341903656721115\n",
      "Epoch 248: train loss: 0.13406822085380554\n",
      "Epoch 249: train loss: 0.13394783437252045\n",
      "Epoch 250: train loss: 0.13382919132709503\n",
      "Epoch 251: train loss: 0.13371221721172333\n",
      "Epoch 252: train loss: 0.13359689712524414\n",
      "Epoch 253: train loss: 0.1334831863641739\n",
      "Epoch 254: train loss: 0.13337109982967377\n",
      "Epoch 255: train loss: 0.1332605630159378\n",
      "Epoch 256: train loss: 0.133151575922966\n",
      "Epoch 257: train loss: 0.1330440789461136\n",
      "Epoch 258: train loss: 0.13293805718421936\n",
      "Epoch 259: train loss: 0.13283348083496094\n",
      "Epoch 260: train loss: 0.13273033499717712\n",
      "Epoch 261: train loss: 0.13262858986854553\n",
      "Epoch 262: train loss: 0.13252821564674377\n",
      "Epoch 263: train loss: 0.13242916762828827\n",
      "Epoch 264: train loss: 0.1323314756155014\n",
      "Epoch 265: train loss: 0.13223505020141602\n",
      "Epoch 266: train loss: 0.1321399211883545\n",
      "Epoch 267: train loss: 0.13204602897167206\n",
      "Epoch 268: train loss: 0.1319533884525299\n",
      "Epoch 269: train loss: 0.13186194002628326\n",
      "Epoch 270: train loss: 0.13177166879177094\n",
      "Epoch 271: train loss: 0.13168257474899292\n",
      "Epoch 272: train loss: 0.13159462809562683\n",
      "Epoch 273: train loss: 0.13150779902935028\n",
      "Epoch 274: train loss: 0.13142207264900208\n",
      "Epoch 275: train loss: 0.13133744895458221\n",
      "Epoch 276: train loss: 0.13125386834144592\n",
      "Epoch 277: train loss: 0.1311713606119156\n",
      "Epoch 278: train loss: 0.13108986616134644\n",
      "Epoch 279: train loss: 0.13100941479206085\n",
      "Epoch 280: train loss: 0.13092991709709167\n",
      "Epoch 281: train loss: 0.13085143268108368\n",
      "Epoch 282: train loss: 0.1307739019393921\n",
      "Epoch 283: train loss: 0.1306973248720169\n",
      "Epoch 284: train loss: 0.13062165677547455\n",
      "Epoch 285: train loss: 0.1305469423532486\n",
      "Epoch 286: train loss: 0.13047310709953308\n",
      "Epoch 287: train loss: 0.1304001659154892\n",
      "Epoch 288: train loss: 0.13032808899879456\n",
      "Epoch 289: train loss: 0.13025687634944916\n",
      "Epoch 290: train loss: 0.13018649816513062\n",
      "Epoch 291: train loss: 0.13011693954467773\n",
      "Epoch 292: train loss: 0.1300482302904129\n",
      "Epoch 293: train loss: 0.12998031079769135\n",
      "Epoch 294: train loss: 0.12991318106651306\n",
      "Epoch 295: train loss: 0.12984684109687805\n",
      "Epoch 296: train loss: 0.12978124618530273\n",
      "Epoch 297: train loss: 0.1297164261341095\n",
      "Epoch 298: train loss: 0.12965233623981476\n",
      "Epoch 299: train loss: 0.12958897650241852\n",
      "Epoch 300: train loss: 0.12952634692192078\n",
      "Epoch 301: train loss: 0.12946440279483795\n",
      "Epoch 302: train loss: 0.12940318882465363\n",
      "Epoch 303: train loss: 0.12934264540672302\n",
      "Epoch 304: train loss: 0.12928277254104614\n",
      "Epoch 305: train loss: 0.1292235553264618\n",
      "Epoch 306: train loss: 0.12916502356529236\n",
      "Epoch 307: train loss: 0.12910713255405426\n",
      "Epoch 308: train loss: 0.1290498673915863\n",
      "Epoch 309: train loss: 0.1289932280778885\n",
      "Epoch 310: train loss: 0.12893721461296082\n",
      "Epoch 311: train loss: 0.1288818120956421\n",
      "Epoch 312: train loss: 0.12882699072360992\n",
      "Epoch 313: train loss: 0.1287727802991867\n",
      "Epoch 314: train loss: 0.12871913611888885\n",
      "Epoch 315: train loss: 0.12866605818271637\n",
      "Epoch 316: train loss: 0.12861356139183044\n",
      "Epoch 317: train loss: 0.12856163084506989\n",
      "Epoch 318: train loss: 0.1285102367401123\n",
      "Epoch 319: train loss: 0.1284593790769577\n",
      "Epoch 320: train loss: 0.12840905785560608\n",
      "Epoch 321: train loss: 0.12835925817489624\n",
      "Epoch 322: train loss: 0.128309965133667\n",
      "Epoch 323: train loss: 0.12826120853424072\n",
      "Epoch 324: train loss: 0.12821294367313385\n",
      "Epoch 325: train loss: 0.12816517055034637\n",
      "Epoch 326: train loss: 0.1281178891658783\n",
      "Epoch 327: train loss: 0.12807109951972961\n",
      "Epoch 328: train loss: 0.12802477180957794\n",
      "Epoch 329: train loss: 0.12797892093658447\n",
      "Epoch 330: train loss: 0.127933531999588\n",
      "Epoch 331: train loss: 0.12788860499858856\n",
      "Epoch 332: train loss: 0.12784411013126373\n",
      "Epoch 333: train loss: 0.12780006229877472\n",
      "Epoch 334: train loss: 0.12775646150112152\n",
      "Epoch 335: train loss: 0.12771329283714294\n",
      "Epoch 336: train loss: 0.127670556306839\n",
      "Epoch 337: train loss: 0.12762820720672607\n",
      "Epoch 338: train loss: 0.12758632004261017\n",
      "Epoch 339: train loss: 0.1275448054075241\n",
      "Epoch 340: train loss: 0.12750370800495148\n",
      "Epoch 341: train loss: 0.12746301293373108\n",
      "Epoch 342: train loss: 0.12742270529270172\n",
      "Epoch 343: train loss: 0.1273827850818634\n",
      "Epoch 344: train loss: 0.12734325230121613\n",
      "Epoch 345: train loss: 0.1273040771484375\n",
      "Epoch 346: train loss: 0.12726528942584991\n",
      "Epoch 347: train loss: 0.12722687423229218\n",
      "Epoch 348: train loss: 0.1271888166666031\n",
      "Epoch 349: train loss: 0.12715110182762146\n",
      "Epoch 350: train loss: 0.12711377441883087\n",
      "Epoch 351: train loss: 0.12707677483558655\n",
      "Epoch 352: train loss: 0.12704011797904968\n",
      "Epoch 353: train loss: 0.12700381875038147\n",
      "Epoch 354: train loss: 0.12696784734725952\n",
      "Epoch 355: train loss: 0.12693218886852264\n",
      "Epoch 356: train loss: 0.12689687311649323\n",
      "Epoch 357: train loss: 0.12686188519001007\n",
      "Epoch 358: train loss: 0.126827210187912\n",
      "Epoch 359: train loss: 0.12679286301136017\n",
      "Epoch 360: train loss: 0.12675881385803223\n",
      "Epoch 361: train loss: 0.12672507762908936\n",
      "Epoch 362: train loss: 0.12669165432453156\n",
      "Epoch 363: train loss: 0.12665851414203644\n",
      "Epoch 364: train loss: 0.1266256719827652\n",
      "Epoch 365: train loss: 0.12659312784671783\n",
      "Epoch 366: train loss: 0.12656086683273315\n",
      "Epoch 367: train loss: 0.12652890384197235\n",
      "Epoch 368: train loss: 0.12649720907211304\n",
      "Epoch 369: train loss: 0.1264658123254776\n",
      "Epoch 370: train loss: 0.12643466889858246\n",
      "Epoch 371: train loss: 0.1264038234949112\n",
      "Epoch 372: train loss: 0.12637321650981903\n",
      "Epoch 373: train loss: 0.12634289264678955\n",
      "Epoch 374: train loss: 0.12631282210350037\n",
      "Epoch 375: train loss: 0.12628303468227386\n",
      "Epoch 376: train loss: 0.12625348567962646\n",
      "Epoch 377: train loss: 0.12622417509555817\n",
      "Epoch 378: train loss: 0.12619514763355255\n",
      "Epoch 379: train loss: 0.12616635859012604\n",
      "Epoch 380: train loss: 0.12613780796527863\n",
      "Epoch 381: train loss: 0.12610948085784912\n",
      "Epoch 382: train loss: 0.1260814219713211\n",
      "Epoch 383: train loss: 0.1260535717010498\n",
      "Epoch 384: train loss: 0.12602598965168\n",
      "Epoch 385: train loss: 0.1259986162185669\n",
      "Epoch 386: train loss: 0.1259714663028717\n",
      "Epoch 387: train loss: 0.1259445697069168\n",
      "Epoch 388: train loss: 0.12591786682605743\n",
      "Epoch 389: train loss: 0.12589138746261597\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 390: train loss: 0.1258651465177536\n",
      "Epoch 391: train loss: 0.12583909928798676\n",
      "Epoch 392: train loss: 0.12581327557563782\n",
      "Epoch 393: train loss: 0.1257876753807068\n",
      "Epoch 394: train loss: 0.12576226890087128\n",
      "Epoch 395: train loss: 0.12573707103729248\n",
      "Epoch 396: train loss: 0.1257120668888092\n",
      "Epoch 397: train loss: 0.12568727135658264\n",
      "Epoch 398: train loss: 0.1256626844406128\n",
      "Epoch 399: train loss: 0.12563827633857727\n",
      "Epoch 400: train loss: 0.12561409175395966\n",
      "Epoch 401: train loss: 0.12559007108211517\n",
      "Epoch 402: train loss: 0.1255662590265274\n",
      "Epoch 403: train loss: 0.12554262578487396\n",
      "Epoch 404: train loss: 0.12551918625831604\n",
      "Epoch 405: train loss: 0.12549594044685364\n",
      "Epoch 406: train loss: 0.12547285854816437\n",
      "Epoch 407: train loss: 0.12544997036457062\n",
      "Epoch 408: train loss: 0.12542724609375\n",
      "Epoch 409: train loss: 0.1254047155380249\n",
      "Epoch 410: train loss: 0.12538234889507294\n",
      "Epoch 411: train loss: 0.1253601610660553\n",
      "Epoch 412: train loss: 0.12533815205097198\n",
      "Epoch 413: train loss: 0.1253162920475006\n",
      "Epoch 414: train loss: 0.12529462575912476\n",
      "Epoch 415: train loss: 0.12527310848236084\n",
      "Epoch 416: train loss: 0.12525175511837006\n",
      "Epoch 417: train loss: 0.1252305656671524\n",
      "Epoch 418: train loss: 0.12520955502986908\n",
      "Epoch 419: train loss: 0.1251886785030365\n",
      "Epoch 420: train loss: 0.12516798079013824\n",
      "Epoch 421: train loss: 0.12514743208885193\n",
      "Epoch 422: train loss: 0.12512701749801636\n",
      "Epoch 423: train loss: 0.1251067817211151\n",
      "Epoch 424: train loss: 0.1250866949558258\n",
      "Epoch 425: train loss: 0.12506674230098724\n",
      "Epoch 426: train loss: 0.12504695355892181\n",
      "Epoch 427: train loss: 0.12502728402614594\n",
      "Epoch 428: train loss: 0.1250077784061432\n",
      "Epoch 429: train loss: 0.12498844414949417\n",
      "Epoch 430: train loss: 0.1249692291021347\n",
      "Epoch 431: train loss: 0.12495014816522598\n",
      "Epoch 432: train loss: 0.1249312087893486\n",
      "Epoch 433: train loss: 0.12491242587566376\n",
      "Epoch 434: train loss: 0.12489375472068787\n",
      "Epoch 435: train loss: 0.12487523257732391\n",
      "Epoch 436: train loss: 0.12485683709383011\n",
      "Epoch 437: train loss: 0.12483858317136765\n",
      "Epoch 438: train loss: 0.12482045590877533\n",
      "Epoch 439: train loss: 0.12480246275663376\n",
      "Epoch 440: train loss: 0.12478459626436234\n",
      "Epoch 441: train loss: 0.12476684898138046\n",
      "Epoch 442: train loss: 0.12474923580884933\n",
      "Epoch 443: train loss: 0.12473175674676895\n",
      "Epoch 444: train loss: 0.12471439689397812\n",
      "Epoch 445: train loss: 0.12469714879989624\n",
      "Epoch 446: train loss: 0.1246800348162651\n",
      "Epoch 447: train loss: 0.12466304004192352\n",
      "Epoch 448: train loss: 0.12464616447687149\n",
      "Epoch 449: train loss: 0.12462940812110901\n",
      "Epoch 450: train loss: 0.12461276352405548\n",
      "Epoch 451: train loss: 0.12459622323513031\n",
      "Epoch 452: train loss: 0.12457982450723648\n",
      "Epoch 453: train loss: 0.12456351518630981\n",
      "Epoch 454: train loss: 0.12454733997583389\n",
      "Epoch 455: train loss: 0.12453126162290573\n",
      "Epoch 456: train loss: 0.12451530247926712\n",
      "Epoch 457: train loss: 0.12449944019317627\n",
      "Epoch 458: train loss: 0.12448371201753616\n",
      "Epoch 459: train loss: 0.12446807324886322\n",
      "Epoch 460: train loss: 0.12445254623889923\n",
      "Epoch 461: train loss: 0.1244371235370636\n",
      "Epoch 462: train loss: 0.12442179024219513\n",
      "Epoch 463: train loss: 0.12440657615661621\n",
      "Epoch 464: train loss: 0.12439146637916565\n",
      "Epoch 465: train loss: 0.12437646836042404\n",
      "Epoch 466: train loss: 0.1243615448474884\n",
      "Epoch 467: train loss: 0.12434674054384232\n",
      "Epoch 468: train loss: 0.12433202564716339\n",
      "Epoch 469: train loss: 0.12431741505861282\n",
      "Epoch 470: train loss: 0.12430290132761002\n",
      "Epoch 471: train loss: 0.12428848445415497\n",
      "Epoch 472: train loss: 0.12427416443824768\n",
      "Epoch 473: train loss: 0.12425993382930756\n",
      "Epoch 474: train loss: 0.12424580007791519\n",
      "Epoch 475: train loss: 0.12423175573348999\n",
      "Epoch 476: train loss: 0.12421780079603195\n",
      "Epoch 477: train loss: 0.12420395016670227\n",
      "Epoch 478: train loss: 0.12419017404317856\n",
      "Epoch 479: train loss: 0.1241765022277832\n",
      "Epoch 480: train loss: 0.12416291236877441\n",
      "Epoch 481: train loss: 0.12414941936731339\n",
      "Epoch 482: train loss: 0.12413600087165833\n",
      "Epoch 483: train loss: 0.12412267923355103\n",
      "Epoch 484: train loss: 0.12410943955183029\n",
      "Epoch 485: train loss: 0.12409628182649612\n",
      "Epoch 486: train loss: 0.12408321350812912\n",
      "Epoch 487: train loss: 0.12407021969556808\n",
      "Epoch 488: train loss: 0.12405732274055481\n",
      "Epoch 489: train loss: 0.1240445077419281\n",
      "Epoch 490: train loss: 0.12403176724910736\n",
      "Epoch 491: train loss: 0.12401910126209259\n",
      "Epoch 492: train loss: 0.12400652468204498\n",
      "Epoch 493: train loss: 0.12399401515722275\n",
      "Epoch 494: train loss: 0.12398160248994827\n",
      "Epoch 495: train loss: 0.12396926432847977\n",
      "Epoch 496: train loss: 0.12395700067281723\n",
      "Epoch 497: train loss: 0.12394480407238007\n",
      "Epoch 498: train loss: 0.12393271178007126\n",
      "Epoch 499: train loss: 0.12392066419124603\n"
     ]
    }
   ],
   "source": [
    "#, bowl_at_ball, model, results, pitch\n",
    "bat_at_ball, bowl_at_ball, model, results, pitch, df = get_embeddings(raw_data_file_name=BBB_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "bat_emb = bat_at_ball.shape[1]\n",
    "bowl_emb = bowl_at_ball.shape[1]\n",
    "\n",
    "\n",
    "all_game_ids = df[\"game_id\"].unique()\n",
    "\n",
    "each_game = {}\n",
    "\n",
    "for game_id in all_game_ids:\n",
    "    indexes = df[\"game_id\"] == game_id\n",
    "    num_in_go = sum(indexes)\n",
    "    \n",
    "    in_values = torch.cat([bat_at_ball[indexes], bowl_at_ball[indexes]], 1).reshape(num_in_go,1, bat_emb+bowl_emb)\n",
    "    \n",
    "    each_game[game_id] = (in_values, results[indexes], pitch[indexes][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0], dtype=int64)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(each_game.values()))[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [1., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [1., 0., 0.,  ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [1., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 1., 0.,  ..., 0., 0., 0.],\n",
       "        [1., 0., 0.,  ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(each_game.values()))[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define RNN model\n",
    "\n",
    "adapted from https://blog.floydhub.com/a-beginners-guide-on-recurrent-neural-networks-with-pytorch/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, bowl_dim, batt_dim, output_size, hidden_dim, n_layers):\n",
    "        super(Model, self).__init__()\n",
    "\n",
    "        # Defining some parameters\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.n_layers = n_layers\n",
    "\n",
    "        #Defining the layers\n",
    "        # RNN Layer\n",
    "    \n",
    "        self.rnn = nn.RNN(bowl_dim + batt_dim, hidden_dim, n_layers, batch_first=True)   \n",
    "        # Fully connected layer\n",
    "        self.fc = nn.Linear(hidden_dim, output_size)\n",
    "        \n",
    "        self.sig = torch.nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \n",
    "        batch_size = x.size(0)\n",
    "\n",
    "        # Initializing hidden state for first input using method defined below\n",
    "        hidden = self.init_hidden(batch_size)\n",
    "\n",
    "        # Passing in the input and hidden state into the model and obtaining outputs\n",
    "        out, hidden = self.rnn(x, hidden)\n",
    "\n",
    "        # Reshaping the outputs such that it can be fit into the fully connected layer\n",
    "        out = out.contiguous().view(-1, self.hidden_dim)\n",
    "        out = self.fc(out)\n",
    "        \n",
    "        return self.sig(out)\n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        # This method generates the first hidden state of zeros which we'll use in the forward pass\n",
    "        # We'll send the tensor holding the hidden state to the device we specified earlier as well\n",
    "        hidden = torch.zeros(self.n_layers, batch_size, self.hidden_dim)\n",
    "        return hidden\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the model with hyperparameters\n",
    "model = Model(bowl_dim = 10, batt_dim=10, output_size=13, hidden_dim=100, n_layers=3)\n",
    "# We'll also set the model to the device that we defined earlier (default is CPU)\n",
    "model.to(device)\n",
    "\n",
    "# Define hyperparameters\n",
    "\n",
    "lr=0.01\n",
    "\n",
    "# Define Loss, Optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.4724, 0.4633, 0.4997,  ..., 0.5361, 0.5150, 0.4683],\n",
       "        [0.4724, 0.4633, 0.4997,  ..., 0.5361, 0.5150, 0.4683],\n",
       "        [0.4724, 0.4633, 0.4997,  ..., 0.5361, 0.5150, 0.4683],\n",
       "        ...,\n",
       "        [0.4712, 0.4621, 0.5011,  ..., 0.5343, 0.5146, 0.4690],\n",
       "        [0.4721, 0.4648, 0.4994,  ..., 0.5356, 0.5141, 0.4687],\n",
       "        [0.4718, 0.4641, 0.4994,  ..., 0.5354, 0.5144, 0.4690]],\n",
       "       grad_fn=<SigmoidBackward0>)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(next(iter(each_game.values()))[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/1000............. "
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'loss' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-18-1728f5d41233>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mepoch\u001b[0m\u001b[1;33m%\u001b[0m\u001b[1;36m10\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Epoch: {}/{}.............'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_epochs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mend\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m' '\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Loss: {:.4f}\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# Clears existing gradients from previous epoch\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'loss' is not defined"
     ]
    }
   ],
   "source": [
    "# Training Run\n",
    "n_epochs = 1000\n",
    "for epoch in range(1, n_epochs + 1):\n",
    "    \n",
    "    optimizer.zero_grad() # Clears existing gradients from previous epoch\n",
    "    #input_seq.to(device)\n",
    "    for embeds, results, ground in each_game.values():\n",
    "        \n",
    "        pred = model(embeds)\n",
    "    \n",
    "        loss = criterion(pred, results)\n",
    "        loss.backward(retain_graph=True) # Does backpropagation and calculates gradients\n",
    "        optimizer.step() # Updates the weights accordingly\n",
    "        \n",
    "    if epoch%10 == 1:\n",
    "        print('Epoch: {}/{}.............'.format(epoch, n_epochs), end=' ')\n",
    "        print(\"Loss: {:.4f}\".format(loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model(next(iter(each_game.values()))[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next(iter(each_game.values()))[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
