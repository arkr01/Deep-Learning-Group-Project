{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([26786, 165]) torch.Size([26786, 165]) torch.Size([26786, 13])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lachl\\anaconda3\\envs\\ml\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3418: DtypeWarning: Columns (14) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "C:\\Users\\lachl\\Google Drive\\Uni Year 5\\git_stuff\\STAT3007Project\\embedding_players.py:47: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  ret_val[wicket_title] = 1 * (ret_val[\"wicket_kind\"] == wicket_title)\n"
     ]
    }
   ],
   "source": [
    "from embedding_players import get_embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU not available, CPU used\n"
     ]
    }
   ],
   "source": [
    "# torch.cuda.is_available() checks and returns a Boolean True if a GPU is available, else it'll return False\n",
    "is_cuda = torch.cuda.is_available()\n",
    "\n",
    "# If we have a GPU available, we'll set our device to GPU. We'll use this device variable later in our code.\n",
    "if is_cuda:\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"GPU is available\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"GPU not available, CPU used\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filepaths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "BBB_filepath = \"C:/Users/lachl/Google Drive/Uni Year 5/git_stuff/STAT3007Project/csv_files/cricsheet_data_partial.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: train loss: 0.6752440929412842\n",
      "Epoch 1: train loss: 0.6713957190513611\n",
      "Epoch 2: train loss: 0.6675800085067749\n",
      "Epoch 3: train loss: 0.6637951731681824\n",
      "Epoch 4: train loss: 0.6600391864776611\n",
      "Epoch 5: train loss: 0.6563103795051575\n",
      "Epoch 6: train loss: 0.6526069045066833\n",
      "Epoch 7: train loss: 0.6489271521568298\n",
      "Epoch 8: train loss: 0.645269513130188\n",
      "Epoch 9: train loss: 0.6416324377059937\n",
      "Epoch 10: train loss: 0.6380143761634827\n",
      "Epoch 11: train loss: 0.6344138979911804\n",
      "Epoch 12: train loss: 0.6308295726776123\n",
      "Epoch 13: train loss: 0.627260148525238\n",
      "Epoch 14: train loss: 0.6237040162086487\n",
      "Epoch 15: train loss: 0.6201599836349487\n",
      "Epoch 16: train loss: 0.6166269183158875\n",
      "Epoch 17: train loss: 0.613103449344635\n",
      "Epoch 18: train loss: 0.6095883846282959\n",
      "Epoch 19: train loss: 0.6060804724693298\n",
      "Epoch 20: train loss: 0.6025786399841309\n",
      "Epoch 21: train loss: 0.5990816950798035\n",
      "Epoch 22: train loss: 0.5955886244773865\n",
      "Epoch 23: train loss: 0.5920981168746948\n",
      "Epoch 24: train loss: 0.5886094570159912\n",
      "Epoch 25: train loss: 0.5851211547851562\n",
      "Epoch 26: train loss: 0.5816326141357422\n",
      "Epoch 27: train loss: 0.5781425833702087\n",
      "Epoch 28: train loss: 0.5746501684188843\n",
      "Epoch 29: train loss: 0.5711544156074524\n",
      "Epoch 30: train loss: 0.5676544308662415\n",
      "Epoch 31: train loss: 0.5641491413116455\n",
      "Epoch 32: train loss: 0.560637891292572\n",
      "Epoch 33: train loss: 0.5571197271347046\n",
      "Epoch 34: train loss: 0.5535938143730164\n",
      "Epoch 35: train loss: 0.55005943775177\n",
      "Epoch 36: train loss: 0.5465156435966492\n",
      "Epoch 37: train loss: 0.542961835861206\n",
      "Epoch 38: train loss: 0.5393973588943481\n",
      "Epoch 39: train loss: 0.5358215570449829\n",
      "Epoch 40: train loss: 0.5322335362434387\n",
      "Epoch 41: train loss: 0.5286329984664917\n",
      "Epoch 42: train loss: 0.5250192880630493\n",
      "Epoch 43: train loss: 0.521391749382019\n",
      "Epoch 44: train loss: 0.5177502036094666\n",
      "Epoch 45: train loss: 0.5140939950942993\n",
      "Epoch 46: train loss: 0.5104228258132935\n",
      "Epoch 47: train loss: 0.5067363381385803\n",
      "Epoch 48: train loss: 0.5030342936515808\n",
      "Epoch 49: train loss: 0.4993164837360382\n",
      "Epoch 50: train loss: 0.4955827295780182\n",
      "Epoch 51: train loss: 0.4918328821659088\n",
      "Epoch 52: train loss: 0.48806697130203247\n",
      "Epoch 53: train loss: 0.48428502678871155\n",
      "Epoch 54: train loss: 0.48048698902130127\n",
      "Epoch 55: train loss: 0.47667309641838074\n",
      "Epoch 56: train loss: 0.4728437066078186\n",
      "Epoch 57: train loss: 0.4689987599849701\n",
      "Epoch 58: train loss: 0.4651389420032501\n",
      "Epoch 59: train loss: 0.4612645208835602\n",
      "Epoch 60: train loss: 0.45737603306770325\n",
      "Epoch 61: train loss: 0.45347392559051514\n",
      "Epoch 62: train loss: 0.44955897331237793\n",
      "Epoch 63: train loss: 0.44563189148902893\n",
      "Epoch 64: train loss: 0.44169339537620544\n",
      "Epoch 65: train loss: 0.4377443790435791\n",
      "Epoch 66: train loss: 0.43378573656082153\n",
      "Epoch 67: train loss: 0.4298184812068939\n",
      "Epoch 68: train loss: 0.42584359645843506\n",
      "Epoch 69: train loss: 0.42186233401298523\n",
      "Epoch 70: train loss: 0.4178759753704071\n",
      "Epoch 71: train loss: 0.41388556361198425\n",
      "Epoch 72: train loss: 0.4098924696445465\n",
      "Epoch 73: train loss: 0.4058981239795685\n",
      "Epoch 74: train loss: 0.4019039571285248\n",
      "Epoch 75: train loss: 0.3979113698005676\n",
      "Epoch 76: train loss: 0.3939219117164612\n",
      "Epoch 77: train loss: 0.38993722200393677\n",
      "Epoch 78: train loss: 0.38595882058143616\n",
      "Epoch 79: train loss: 0.38198843598365784\n",
      "Epoch 80: train loss: 0.37802764773368835\n",
      "Epoch 81: train loss: 0.37407806515693665\n",
      "Epoch 82: train loss: 0.37014156579971313\n",
      "Epoch 83: train loss: 0.36621972918510437\n",
      "Epoch 84: train loss: 0.36231428384780884\n",
      "Epoch 85: train loss: 0.3584270179271698\n",
      "Epoch 86: train loss: 0.35455960035324097\n",
      "Epoch 87: train loss: 0.35071370005607605\n",
      "Epoch 88: train loss: 0.3468910753726959\n",
      "Epoch 89: train loss: 0.3430933654308319\n",
      "Epoch 90: train loss: 0.33932217955589294\n",
      "Epoch 91: train loss: 0.33557915687561035\n",
      "Epoch 92: train loss: 0.3318658769130707\n",
      "Epoch 93: train loss: 0.3281838595867157\n",
      "Epoch 94: train loss: 0.3245345652103424\n",
      "Epoch 95: train loss: 0.3209194839000702\n",
      "Epoch 96: train loss: 0.3173399567604065\n",
      "Epoch 97: train loss: 0.3137972950935364\n",
      "Epoch 98: train loss: 0.3102928102016449\n",
      "Epoch 99: train loss: 0.30682772397994995\n",
      "Epoch 100: train loss: 0.3034031093120575\n",
      "Epoch 101: train loss: 0.30002012848854065\n",
      "Epoch 102: train loss: 0.29667967557907104\n",
      "Epoch 103: train loss: 0.29338276386260986\n",
      "Epoch 104: train loss: 0.2901301681995392\n",
      "Epoch 105: train loss: 0.28692275285720825\n",
      "Epoch 106: train loss: 0.28376123309135437\n",
      "Epoch 107: train loss: 0.2806461751461029\n",
      "Epoch 108: train loss: 0.2775782346725464\n",
      "Epoch 109: train loss: 0.27455779910087585\n",
      "Epoch 110: train loss: 0.2715853452682495\n",
      "Epoch 111: train loss: 0.268661230802536\n",
      "Epoch 112: train loss: 0.2657857835292816\n",
      "Epoch 113: train loss: 0.2629590928554535\n",
      "Epoch 114: train loss: 0.26018139719963074\n",
      "Epoch 115: train loss: 0.25745275616645813\n",
      "Epoch 116: train loss: 0.25477319955825806\n",
      "Epoch 117: train loss: 0.25214263796806335\n",
      "Epoch 118: train loss: 0.24956102669239044\n",
      "Epoch 119: train loss: 0.2470281720161438\n",
      "Epoch 120: train loss: 0.2445439100265503\n",
      "Epoch 121: train loss: 0.24210797250270844\n",
      "Epoch 122: train loss: 0.23972003161907196\n",
      "Epoch 123: train loss: 0.2373797446489334\n",
      "Epoch 124: train loss: 0.23508675396442413\n",
      "Epoch 125: train loss: 0.23284058272838593\n",
      "Epoch 126: train loss: 0.23064081370830536\n",
      "Epoch 127: train loss: 0.22848694026470184\n",
      "Epoch 128: train loss: 0.2263784408569336\n",
      "Epoch 129: train loss: 0.22431467473506927\n",
      "Epoch 130: train loss: 0.22229519486427307\n",
      "Epoch 131: train loss: 0.2203192561864853\n",
      "Epoch 132: train loss: 0.21838636696338654\n",
      "Epoch 133: train loss: 0.21649578213691711\n",
      "Epoch 134: train loss: 0.2146468609571457\n",
      "Epoch 135: train loss: 0.21283894777297974\n",
      "Epoch 136: train loss: 0.21107132732868195\n",
      "Epoch 137: train loss: 0.20934335887432098\n",
      "Epoch 138: train loss: 0.20765426754951477\n",
      "Epoch 139: train loss: 0.2060033529996872\n",
      "Epoch 140: train loss: 0.20438992977142334\n",
      "Epoch 141: train loss: 0.20281323790550232\n",
      "Epoch 142: train loss: 0.20127257704734802\n",
      "Epoch 143: train loss: 0.19976723194122314\n",
      "Epoch 144: train loss: 0.1982964426279068\n",
      "Epoch 145: train loss: 0.19685950875282288\n",
      "Epoch 146: train loss: 0.19545572996139526\n",
      "Epoch 147: train loss: 0.19408437609672546\n",
      "Epoch 148: train loss: 0.1927446871995926\n",
      "Epoch 149: train loss: 0.1914360672235489\n",
      "Epoch 150: train loss: 0.1901577264070511\n",
      "Epoch 151: train loss: 0.18890902400016785\n",
      "Epoch 152: train loss: 0.18768927454948425\n",
      "Epoch 153: train loss: 0.1864977777004242\n",
      "Epoch 154: train loss: 0.18533390760421753\n",
      "Epoch 155: train loss: 0.18419697880744934\n",
      "Epoch 156: train loss: 0.1830863654613495\n",
      "Epoch 157: train loss: 0.18200144171714783\n",
      "Epoch 158: train loss: 0.18094156682491302\n",
      "Epoch 159: train loss: 0.17990612983703613\n",
      "Epoch 160: train loss: 0.1788945496082306\n",
      "Epoch 161: train loss: 0.17790624499320984\n",
      "Epoch 162: train loss: 0.17694060504436493\n",
      "Epoch 163: train loss: 0.1759970635175705\n",
      "Epoch 164: train loss: 0.17507511377334595\n",
      "Epoch 165: train loss: 0.17417415976524353\n",
      "Epoch 166: train loss: 0.17329370975494385\n",
      "Epoch 167: train loss: 0.17243322730064392\n",
      "Epoch 168: train loss: 0.17159217596054077\n",
      "Epoch 169: train loss: 0.17077012360095978\n",
      "Epoch 170: train loss: 0.16996654868125916\n",
      "Epoch 171: train loss: 0.1691809743642807\n",
      "Epoch 172: train loss: 0.1684129685163498\n",
      "Epoch 173: train loss: 0.16766203939914703\n",
      "Epoch 174: train loss: 0.1669277846813202\n",
      "Epoch 175: train loss: 0.16620974242687225\n",
      "Epoch 176: train loss: 0.16550752520561218\n",
      "Epoch 177: train loss: 0.16482071578502655\n",
      "Epoch 178: train loss: 0.16414889693260193\n",
      "Epoch 179: train loss: 0.16349171102046967\n",
      "Epoch 180: train loss: 0.16284877061843872\n",
      "Epoch 181: train loss: 0.16221971809864044\n",
      "Epoch 182: train loss: 0.16160418093204498\n",
      "Epoch 183: train loss: 0.16100183129310608\n",
      "Epoch 184: train loss: 0.16041229665279388\n",
      "Epoch 185: train loss: 0.15983527898788452\n",
      "Epoch 186: train loss: 0.15927045047283173\n",
      "Epoch 187: train loss: 0.15871749818325043\n",
      "Epoch 188: train loss: 0.15817609429359436\n",
      "Epoch 189: train loss: 0.15764600038528442\n",
      "Epoch 190: train loss: 0.15712687373161316\n",
      "Epoch 191: train loss: 0.15661846101284027\n",
      "Epoch 192: train loss: 0.15612049400806427\n",
      "Epoch 193: train loss: 0.15563270449638367\n",
      "Epoch 194: train loss: 0.15515482425689697\n",
      "Epoch 195: train loss: 0.1546865999698639\n",
      "Epoch 196: train loss: 0.1542278230190277\n",
      "Epoch 197: train loss: 0.15377822518348694\n",
      "Epoch 198: train loss: 0.15333756804466248\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 199: train loss: 0.15290570259094238\n",
      "Epoch 200: train loss: 0.1524822860956192\n",
      "Epoch 201: train loss: 0.15206721425056458\n",
      "Epoch 202: train loss: 0.1516602486371994\n",
      "Epoch 203: train loss: 0.15126118063926697\n",
      "Epoch 204: train loss: 0.15086981654167175\n",
      "Epoch 205: train loss: 0.15048599243164062\n",
      "Epoch 206: train loss: 0.15010949969291687\n",
      "Epoch 207: train loss: 0.14974014461040497\n",
      "Epoch 208: train loss: 0.14937780797481537\n",
      "Epoch 209: train loss: 0.14902228116989136\n",
      "Epoch 210: train loss: 0.1486733853816986\n",
      "Epoch 211: train loss: 0.14833101630210876\n",
      "Epoch 212: train loss: 0.1479949802160263\n",
      "Epoch 213: train loss: 0.1476651281118393\n",
      "Epoch 214: train loss: 0.14734134078025818\n",
      "Epoch 215: train loss: 0.14702342450618744\n",
      "Epoch 216: train loss: 0.1467113047838211\n",
      "Epoch 217: train loss: 0.14640478789806366\n",
      "Epoch 218: train loss: 0.14610376954078674\n",
      "Epoch 219: train loss: 0.1458081156015396\n",
      "Epoch 220: train loss: 0.1455177217721939\n",
      "Epoch 221: train loss: 0.1452324539422989\n",
      "Epoch 222: train loss: 0.144952192902565\n",
      "Epoch 223: train loss: 0.1446768194437027\n",
      "Epoch 224: train loss: 0.1444062441587448\n",
      "Epoch 225: train loss: 0.1441403180360794\n",
      "Epoch 226: train loss: 0.1438789814710617\n",
      "Epoch 227: train loss: 0.14362211525440216\n",
      "Epoch 228: train loss: 0.14336960017681122\n",
      "Epoch 229: train loss: 0.1431213766336441\n",
      "Epoch 230: train loss: 0.14287731051445007\n",
      "Epoch 231: train loss: 0.14263732731342316\n",
      "Epoch 232: train loss: 0.1424013376235962\n",
      "Epoch 233: train loss: 0.1421692669391632\n",
      "Epoch 234: train loss: 0.14194101095199585\n",
      "Epoch 235: train loss: 0.14171649515628815\n",
      "Epoch 236: train loss: 0.14149564504623413\n",
      "Epoch 237: train loss: 0.14127837121486664\n",
      "Epoch 238: train loss: 0.1410645991563797\n",
      "Epoch 239: train loss: 0.14085425436496735\n",
      "Epoch 240: train loss: 0.1406472772359848\n",
      "Epoch 241: train loss: 0.1404435634613037\n",
      "Epoch 242: train loss: 0.14024309813976288\n",
      "Epoch 243: train loss: 0.14004577696323395\n",
      "Epoch 244: train loss: 0.13985152542591095\n",
      "Epoch 245: train loss: 0.1396603137254715\n",
      "Epoch 246: train loss: 0.13947203755378723\n",
      "Epoch 247: train loss: 0.13928668200969696\n",
      "Epoch 248: train loss: 0.13910415768623352\n",
      "Epoch 249: train loss: 0.13892440497875214\n",
      "Epoch 250: train loss: 0.13874737918376923\n",
      "Epoch 251: train loss: 0.1385730355978012\n",
      "Epoch 252: train loss: 0.1384013146162033\n",
      "Epoch 253: train loss: 0.13823212683200836\n",
      "Epoch 254: train loss: 0.13806548714637756\n",
      "Epoch 255: train loss: 0.13790129125118256\n",
      "Epoch 256: train loss: 0.13773952424526215\n",
      "Epoch 257: train loss: 0.13758011162281036\n",
      "Epoch 258: train loss: 0.13742302358150482\n",
      "Epoch 259: train loss: 0.13726820051670074\n",
      "Epoch 260: train loss: 0.13711564242839813\n",
      "Epoch 261: train loss: 0.13696525990962982\n",
      "Epoch 262: train loss: 0.13681700825691223\n",
      "Epoch 263: train loss: 0.13667088747024536\n",
      "Epoch 264: train loss: 0.13652680814266205\n",
      "Epoch 265: train loss: 0.1363847851753235\n",
      "Epoch 266: train loss: 0.1362447291612625\n",
      "Epoch 267: train loss: 0.13610664010047913\n",
      "Epoch 268: train loss: 0.13597045838832855\n",
      "Epoch 269: train loss: 0.1358361691236496\n",
      "Epoch 270: train loss: 0.13570371270179749\n",
      "Epoch 271: train loss: 0.13557307422161102\n",
      "Epoch 272: train loss: 0.13544422388076782\n",
      "Epoch 273: train loss: 0.1353171020746231\n",
      "Epoch 274: train loss: 0.13519169390201569\n",
      "Epoch 275: train loss: 0.13506798446178436\n",
      "Epoch 276: train loss: 0.13494591414928436\n",
      "Epoch 277: train loss: 0.13482549786567688\n",
      "Epoch 278: train loss: 0.13470663130283356\n",
      "Epoch 279: train loss: 0.13458934426307678\n",
      "Epoch 280: train loss: 0.13447360694408417\n",
      "Epoch 281: train loss: 0.13435937464237213\n",
      "Epoch 282: train loss: 0.13424663245677948\n",
      "Epoch 283: train loss: 0.13413533568382263\n",
      "Epoch 284: train loss: 0.1340254843235016\n",
      "Epoch 285: train loss: 0.13391703367233276\n",
      "Epoch 286: train loss: 0.13380996882915497\n",
      "Epoch 287: train loss: 0.133704274892807\n",
      "Epoch 288: train loss: 0.1335999071598053\n",
      "Epoch 289: train loss: 0.13349685072898865\n",
      "Epoch 290: train loss: 0.13339510560035706\n",
      "Epoch 291: train loss: 0.13329462707042694\n",
      "Epoch 292: train loss: 0.13319537043571472\n",
      "Epoch 293: train loss: 0.13309736549854279\n",
      "Epoch 294: train loss: 0.13300058245658875\n",
      "Epoch 295: train loss: 0.13290496170520782\n",
      "Epoch 296: train loss: 0.13281050324440002\n",
      "Epoch 297: train loss: 0.13271722197532654\n",
      "Epoch 298: train loss: 0.1326250433921814\n",
      "Epoch 299: train loss: 0.1325339823961258\n",
      "Epoch 300: train loss: 0.13244402408599854\n",
      "Epoch 301: train loss: 0.13235512375831604\n",
      "Epoch 302: train loss: 0.1322673112154007\n",
      "Epoch 303: train loss: 0.13218051195144653\n",
      "Epoch 304: train loss: 0.13209475576877594\n",
      "Epoch 305: train loss: 0.13200999796390533\n",
      "Epoch 306: train loss: 0.13192623853683472\n",
      "Epoch 307: train loss: 0.1318434625864029\n",
      "Epoch 308: train loss: 0.13176164031028748\n",
      "Epoch 309: train loss: 0.13168074190616608\n",
      "Epoch 310: train loss: 0.13160079717636108\n",
      "Epoch 311: train loss: 0.13152176141738892\n",
      "Epoch 312: train loss: 0.13144363462924957\n",
      "Epoch 313: train loss: 0.13136638700962067\n",
      "Epoch 314: train loss: 0.1312900334596634\n",
      "Epoch 315: train loss: 0.13121452927589417\n",
      "Epoch 316: train loss: 0.131139874458313\n",
      "Epoch 317: train loss: 0.13106605410575867\n",
      "Epoch 318: train loss: 0.13099305331707\n",
      "Epoch 319: train loss: 0.13092085719108582\n",
      "Epoch 320: train loss: 0.1308494657278061\n",
      "Epoch 321: train loss: 0.13077887892723083\n",
      "Epoch 322: train loss: 0.13070905208587646\n",
      "Epoch 323: train loss: 0.1306399703025818\n",
      "Epoch 324: train loss: 0.13057167828083038\n",
      "Epoch 325: train loss: 0.13050410151481628\n",
      "Epoch 326: train loss: 0.13043725490570068\n",
      "Epoch 327: train loss: 0.13037113845348358\n",
      "Epoch 328: train loss: 0.13030573725700378\n",
      "Epoch 329: train loss: 0.1302410215139389\n",
      "Epoch 330: train loss: 0.13017697632312775\n",
      "Epoch 331: train loss: 0.1301136314868927\n",
      "Epoch 332: train loss: 0.13005095720291138\n",
      "Epoch 333: train loss: 0.1299889236688614\n",
      "Epoch 334: train loss: 0.12992756068706512\n",
      "Epoch 335: train loss: 0.1298668384552002\n",
      "Epoch 336: train loss: 0.1298067420721054\n",
      "Epoch 337: train loss: 0.12974725663661957\n",
      "Epoch 338: train loss: 0.12968838214874268\n",
      "Epoch 339: train loss: 0.12963013350963593\n",
      "Epoch 340: train loss: 0.12957248091697693\n",
      "Epoch 341: train loss: 0.1295153945684433\n",
      "Epoch 342: train loss: 0.12945890426635742\n",
      "Epoch 343: train loss: 0.1294029951095581\n",
      "Epoch 344: train loss: 0.12934763729572296\n",
      "Epoch 345: train loss: 0.12929284572601318\n",
      "Epoch 346: train loss: 0.12923859059810638\n",
      "Epoch 347: train loss: 0.12918488681316376\n",
      "Epoch 348: train loss: 0.1291317194700241\n",
      "Epoch 349: train loss: 0.12907907366752625\n",
      "Epoch 350: train loss: 0.12902694940567017\n",
      "Epoch 351: train loss: 0.12897533178329468\n",
      "Epoch 352: train loss: 0.12892425060272217\n",
      "Epoch 353: train loss: 0.12887363135814667\n",
      "Epoch 354: train loss: 0.12882351875305176\n",
      "Epoch 355: train loss: 0.12877388298511505\n",
      "Epoch 356: train loss: 0.12872473895549774\n",
      "Epoch 357: train loss: 0.12867605686187744\n",
      "Epoch 358: train loss: 0.12862785160541534\n",
      "Epoch 359: train loss: 0.12858010828495026\n",
      "Epoch 360: train loss: 0.12853282690048218\n",
      "Epoch 361: train loss: 0.12848599255084991\n",
      "Epoch 362: train loss: 0.12843960523605347\n",
      "Epoch 363: train loss: 0.12839363515377045\n",
      "Epoch 364: train loss: 0.12834811210632324\n",
      "Epoch 365: train loss: 0.12830300629138947\n",
      "Epoch 366: train loss: 0.1282583326101303\n",
      "Epoch 367: train loss: 0.12821407616138458\n",
      "Epoch 368: train loss: 0.1281702220439911\n",
      "Epoch 369: train loss: 0.12812678515911102\n",
      "Epoch 370: train loss: 0.12808376550674438\n",
      "Epoch 371: train loss: 0.1280411034822464\n",
      "Epoch 372: train loss: 0.12799885869026184\n",
      "Epoch 373: train loss: 0.12795700132846832\n",
      "Epoch 374: train loss: 0.12791548669338226\n",
      "Epoch 375: train loss: 0.12787438929080963\n",
      "Epoch 376: train loss: 0.12783364951610565\n",
      "Epoch 377: train loss: 0.12779326736927032\n",
      "Epoch 378: train loss: 0.12775325775146484\n",
      "Epoch 379: train loss: 0.1277136206626892\n",
      "Epoch 380: train loss: 0.12767431139945984\n",
      "Epoch 381: train loss: 0.12763535976409912\n",
      "Epoch 382: train loss: 0.12759676575660706\n",
      "Epoch 383: train loss: 0.12755849957466125\n",
      "Epoch 384: train loss: 0.1275206059217453\n",
      "Epoch 385: train loss: 0.12748299539089203\n",
      "Epoch 386: train loss: 0.12744572758674622\n",
      "Epoch 387: train loss: 0.12740880250930786\n",
      "Epoch 388: train loss: 0.12737217545509338\n",
      "Epoch 389: train loss: 0.12733590602874756\n",
      "Epoch 390: train loss: 0.12729990482330322\n",
      "Epoch 391: train loss: 0.12726424634456635\n",
      "Epoch 392: train loss: 0.12722887098789215\n",
      "Epoch 393: train loss: 0.12719380855560303\n",
      "Epoch 394: train loss: 0.12715904414653778\n",
      "Epoch 395: train loss: 0.1271245926618576\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 396: train loss: 0.12709040939807892\n",
      "Epoch 397: train loss: 0.1270565241575241\n",
      "Epoch 398: train loss: 0.12702293694019318\n",
      "Epoch 399: train loss: 0.12698963284492493\n",
      "Epoch 400: train loss: 0.12695659697055817\n",
      "Epoch 401: train loss: 0.1269238293170929\n",
      "Epoch 402: train loss: 0.1268913447856903\n",
      "Epoch 403: train loss: 0.1268591284751892\n",
      "Epoch 404: train loss: 0.1268271803855896\n",
      "Epoch 405: train loss: 0.12679550051689148\n",
      "Epoch 406: train loss: 0.12676408886909485\n",
      "Epoch 407: train loss: 0.12673290073871613\n",
      "Epoch 408: train loss: 0.12670201063156128\n",
      "Epoch 409: train loss: 0.12667137384414673\n",
      "Epoch 410: train loss: 0.1266409456729889\n",
      "Epoch 411: train loss: 0.12661078572273254\n",
      "Epoch 412: train loss: 0.1265808790922165\n",
      "Epoch 413: train loss: 0.12655121088027954\n",
      "Epoch 414: train loss: 0.1265217661857605\n",
      "Epoch 415: train loss: 0.12649257481098175\n",
      "Epoch 416: train loss: 0.1264636069536209\n",
      "Epoch 417: train loss: 0.12643487751483917\n",
      "Epoch 418: train loss: 0.12640638649463654\n",
      "Epoch 419: train loss: 0.1263781040906906\n",
      "Epoch 420: train loss: 0.1263500601053238\n",
      "Epoch 421: train loss: 0.12632222473621368\n",
      "Epoch 422: train loss: 0.1262945979833603\n",
      "Epoch 423: train loss: 0.1262672245502472\n",
      "Epoch 424: train loss: 0.1262400597333908\n",
      "Epoch 425: train loss: 0.12621308863162994\n",
      "Epoch 426: train loss: 0.1261863261461258\n",
      "Epoch 427: train loss: 0.12615978717803955\n",
      "Epoch 428: train loss: 0.12613342702388763\n",
      "Epoch 429: train loss: 0.12610729038715363\n",
      "Epoch 430: train loss: 0.12608134746551514\n",
      "Epoch 431: train loss: 0.12605561316013336\n",
      "Epoch 432: train loss: 0.1260300725698471\n",
      "Epoch 433: train loss: 0.12600474059581757\n",
      "Epoch 434: train loss: 0.12597958743572235\n",
      "Epoch 435: train loss: 0.12595461308956146\n",
      "Epoch 436: train loss: 0.12592986226081848\n",
      "Epoch 437: train loss: 0.12590526044368744\n",
      "Epoch 438: train loss: 0.1258808672428131\n",
      "Epoch 439: train loss: 0.1258566677570343\n",
      "Epoch 440: train loss: 0.12583263218402863\n",
      "Epoch 441: train loss: 0.12580876052379608\n",
      "Epoch 442: train loss: 0.12578509747982025\n",
      "Epoch 443: train loss: 0.12576159834861755\n",
      "Epoch 444: train loss: 0.12573827803134918\n",
      "Epoch 445: train loss: 0.12571510672569275\n",
      "Epoch 446: train loss: 0.12569214403629303\n",
      "Epoch 447: train loss: 0.12566933035850525\n",
      "Epoch 448: train loss: 0.1256466805934906\n",
      "Epoch 449: train loss: 0.12562420964241028\n",
      "Epoch 450: train loss: 0.1256019026041031\n",
      "Epoch 451: train loss: 0.12557974457740784\n",
      "Epoch 452: train loss: 0.1255577653646469\n",
      "Epoch 453: train loss: 0.12553595006465912\n",
      "Epoch 454: train loss: 0.12551425397396088\n",
      "Epoch 455: train loss: 0.12549275159835815\n",
      "Epoch 456: train loss: 0.12547139823436737\n",
      "Epoch 457: train loss: 0.12545017898082733\n",
      "Epoch 458: train loss: 0.12542912364006042\n",
      "Epoch 459: train loss: 0.12540823221206665\n",
      "Epoch 460: train loss: 0.12538745999336243\n",
      "Epoch 461: train loss: 0.12536686658859253\n",
      "Epoch 462: train loss: 0.12534640729427338\n",
      "Epoch 463: train loss: 0.12532609701156616\n",
      "Epoch 464: train loss: 0.1253059208393097\n",
      "Epoch 465: train loss: 0.12528589367866516\n",
      "Epoch 466: train loss: 0.12526601552963257\n",
      "Epoch 467: train loss: 0.12524625658988953\n",
      "Epoch 468: train loss: 0.12522664666175842\n",
      "Epoch 469: train loss: 0.12520717084407806\n",
      "Epoch 470: train loss: 0.12518782913684845\n",
      "Epoch 471: train loss: 0.12516862154006958\n",
      "Epoch 472: train loss: 0.12514956295490265\n",
      "Epoch 473: train loss: 0.12513062357902527\n",
      "Epoch 474: train loss: 0.12511180341243744\n",
      "Epoch 475: train loss: 0.12509313225746155\n",
      "Epoch 476: train loss: 0.1250745803117752\n",
      "Epoch 477: train loss: 0.1250561624765396\n",
      "Epoch 478: train loss: 0.12503784894943237\n",
      "Epoch 479: train loss: 0.12501966953277588\n",
      "Epoch 480: train loss: 0.12500163912773132\n",
      "Epoch 481: train loss: 0.12498371303081512\n",
      "Epoch 482: train loss: 0.12496589869260788\n",
      "Epoch 483: train loss: 0.12494821846485138\n",
      "Epoch 484: train loss: 0.12493064999580383\n",
      "Epoch 485: train loss: 0.12491319328546524\n",
      "Epoch 486: train loss: 0.12489587813615799\n",
      "Epoch 487: train loss: 0.1248786598443985\n",
      "Epoch 488: train loss: 0.12486154586076736\n",
      "Epoch 489: train loss: 0.12484456598758698\n",
      "Epoch 490: train loss: 0.12482769042253494\n",
      "Epoch 491: train loss: 0.12481093406677246\n",
      "Epoch 492: train loss: 0.12479428946971893\n",
      "Epoch 493: train loss: 0.12477774918079376\n",
      "Epoch 494: train loss: 0.12476132065057755\n",
      "Epoch 495: train loss: 0.12474499642848969\n",
      "Epoch 496: train loss: 0.12472877651453018\n",
      "Epoch 497: train loss: 0.12471265345811844\n",
      "Epoch 498: train loss: 0.12469664961099625\n",
      "Epoch 499: train loss: 0.12468075007200241\n"
     ]
    }
   ],
   "source": [
    "#, bowl_at_ball, model, results, pitch\n",
    "bat_at_ball, bowl_at_ball, model, results, pitch, df = get_embeddings(raw_data_file_name=BBB_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "bat_emb = bat_at_ball.shape[1]\n",
    "bowl_emb = bowl_at_ball.shape[1]\n",
    "\n",
    "\n",
    "all_game_ids = df[\"game_id\"].unique()\n",
    "\n",
    "each_game = {}\n",
    "\n",
    "for game_id in all_game_ids:\n",
    "    indexes = df[\"game_id\"] == game_id\n",
    "    num_in_go = sum(indexes)\n",
    "    \n",
    "    in_values = torch.cat([bat_at_ball[indexes], bowl_at_ball[indexes]], 1).reshape(num_in_go,1, bat_emb+bowl_emb)\n",
    "    \n",
    "    each_game[game_id] = (in_values, results[indexes], pitch[indexes])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.5102,  1.1968, -0.2728,  ...,  2.1892,  0.6875,  0.0693]],\n",
       "\n",
       "        [[-0.5102,  1.1968, -0.2728,  ...,  2.1892,  0.6875,  0.0693]],\n",
       "\n",
       "        [[-0.5102,  1.1968, -0.2728,  ...,  2.1892,  0.6875,  0.0693]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[-0.5067,  1.2022, -0.3001,  ...,  2.2395,  0.6769,  0.0490]],\n",
       "\n",
       "        [[-0.5992,  1.1219, -0.2616,  ...,  2.1535,  0.6816,  0.1065]],\n",
       "\n",
       "        [[-0.5258,  1.1834, -0.3055,  ...,  2.1535,  0.6816,  0.1065]]],\n",
       "       grad_fn=<ReshapeAliasBackward0>)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(each_game.values()))[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define RNN model\n",
    "\n",
    "adapted from https://blog.floydhub.com/a-beginners-guide-on-recurrent-neural-networks-with-pytorch/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, bowl_dim, batt_dim, output_size, hidden_dim, n_layers):\n",
    "        super(Model, self).__init__()\n",
    "\n",
    "        # Defining some parameters\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.n_layers = n_layers\n",
    "\n",
    "        #Defining the layers\n",
    "        # RNN Layer\n",
    "    \n",
    "        self.rnn = nn.RNN(bowl_dim + batt_dim, hidden_dim, n_layers, batch_first=True)   \n",
    "        # Fully connected layer\n",
    "        self.fc = nn.Linear(hidden_dim, output_size)\n",
    "        \n",
    "        self.sig = torch.nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \n",
    "        batch_size = x.size(0)\n",
    "\n",
    "        # Initializing hidden state for first input using method defined below\n",
    "        hidden = self.init_hidden(batch_size)\n",
    "\n",
    "        # Passing in the input and hidden state into the model and obtaining outputs\n",
    "        out, hidden = self.rnn(x, hidden)\n",
    "\n",
    "        # Reshaping the outputs such that it can be fit into the fully connected layer\n",
    "        out = out.contiguous().view(-1, self.hidden_dim)\n",
    "        out = self.fc(out)\n",
    "        \n",
    "        return self.sig(out)\n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        # This method generates the first hidden state of zeros which we'll use in the forward pass\n",
    "        # We'll send the tensor holding the hidden state to the device we specified earlier as well\n",
    "        hidden = torch.zeros(self.n_layers, batch_size, self.hidden_dim)\n",
    "        return hidden\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the model with hyperparameters\n",
    "model = Model(bowl_dim = 10, batt_dim=10, output_size=13, hidden_dim=100, n_layers=3)\n",
    "# We'll also set the model to the device that we defined earlier (default is CPU)\n",
    "model.to(device)\n",
    "\n",
    "# Define hyperparameters\n",
    "\n",
    "lr=0.01\n",
    "\n",
    "# Define Loss, Optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.4873, 0.4760, 0.4989,  ..., 0.5212, 0.4879, 0.5319],\n",
       "        [0.4873, 0.4760, 0.4989,  ..., 0.5212, 0.4879, 0.5319],\n",
       "        [0.4873, 0.4760, 0.4989,  ..., 0.5212, 0.4879, 0.5319],\n",
       "        ...,\n",
       "        [0.4881, 0.4760, 0.4996,  ..., 0.5204, 0.4881, 0.5321],\n",
       "        [0.4871, 0.4750, 0.4996,  ..., 0.5192, 0.4878, 0.5315],\n",
       "        [0.4874, 0.4754, 0.4987,  ..., 0.5196, 0.4868, 0.5324]],\n",
       "       grad_fn=<SigmoidBackward0>)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(next(iter(each_game.values()))[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/1000............. Loss: 0.0332\n",
      "Epoch: 2/1000............. Loss: 0.2434\n",
      "Epoch: 3/1000............. Loss: 0.2282\n",
      "Epoch: 4/1000............. Loss: 0.2129\n",
      "Epoch: 5/1000............. Loss: 0.1966\n",
      "Epoch: 6/1000............. Loss: 0.1784\n",
      "Epoch: 7/1000............. Loss: 0.1585\n",
      "Epoch: 8/1000............. Loss: 0.1374\n",
      "Epoch: 9/1000............. Loss: 0.1166\n",
      "Epoch: 10/1000............. Loss: 0.0980\n",
      "Epoch: 11/1000............. Loss: 0.0827\n",
      "Epoch: 12/1000............. Loss: 0.0710\n",
      "Epoch: 13/1000............. Loss: 0.0625\n",
      "Epoch: 14/1000............. Loss: 0.0563\n",
      "Epoch: 15/1000............. Loss: 0.0518\n",
      "Epoch: 16/1000............. Loss: 0.0485\n",
      "Epoch: 17/1000............. Loss: 0.0460\n",
      "Epoch: 18/1000............. Loss: 0.0441\n",
      "Epoch: 19/1000............. Loss: 0.0426\n",
      "Epoch: 20/1000............. Loss: 0.0414\n",
      "Epoch: 21/1000............. Loss: 0.0404\n",
      "Epoch: 22/1000............. Loss: 0.0397\n",
      "Epoch: 23/1000............. Loss: 0.0390\n",
      "Epoch: 24/1000............. Loss: 0.0385\n",
      "Epoch: 25/1000............. Loss: 0.0380\n",
      "Epoch: 26/1000............. Loss: 0.0376\n",
      "Epoch: 27/1000............. Loss: 0.0373\n",
      "Epoch: 28/1000............. Loss: 0.0370\n",
      "Epoch: 29/1000............. Loss: 0.0367\n",
      "Epoch: 30/1000............. Loss: 0.0365\n",
      "Epoch: 31/1000............. Loss: 0.0363\n",
      "Epoch: 32/1000............. Loss: 0.0361\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-68-64714a5defbf>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0membeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresults\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mground\u001b[0m \u001b[1;32min\u001b[0m \u001b[0meach_game\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m         \u001b[0mpred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0membeds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m         \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresults\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\ml\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1103\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-65-982cb183b2a1>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m         \u001b[1;31m# Passing in the input and hidden state into the model and obtaining outputs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 26\u001b[1;33m         \u001b[0mout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhidden\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrnn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     27\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m         \u001b[1;31m# Reshaping the outputs such that it can be fit into the fully connected layer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\ml\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1103\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\ml\\lib\\site-packages\\torch\\nn\\modules\\rnn.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input, hx)\u001b[0m\n\u001b[0;32m    266\u001b[0m         \u001b[0m_impl\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_rnn_impls\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    267\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mbatch_sizes\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 268\u001b[1;33m             result = _impl(input, hx, self._flat_weights, self.bias, self.num_layers,\n\u001b[0m\u001b[0;32m    269\u001b[0m                            self.dropout, self.training, self.bidirectional, self.batch_first)\n\u001b[0;32m    270\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Training Run\n",
    "n_epochs = 1000\n",
    "for epoch in range(1, n_epochs + 1):\n",
    "    if epoch%1 == 0:\n",
    "        print('Epoch: {}/{}.............'.format(epoch, n_epochs), end=' ')\n",
    "        print(\"Loss: {:.4f}\".format(loss.item()))\n",
    "    \n",
    "    optimizer.zero_grad() # Clears existing gradients from previous epoch\n",
    "    #input_seq.to(device)\n",
    "    for embeds, results, ground in each_game.values():\n",
    "        \n",
    "        pred = model(embeds)\n",
    "    \n",
    "        loss = criterion(pred, results)\n",
    "        loss.backward(retain_graph=True) # Does backpropagation and calculates gradients\n",
    "        optimizer.step() # Updates the weights accordingly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.7352, 0.1507, 0.0345,  ..., 0.0214, 0.0207, 0.0211],\n",
       "        [0.7352, 0.1507, 0.0345,  ..., 0.0214, 0.0207, 0.0211],\n",
       "        [0.7352, 0.1507, 0.0345,  ..., 0.0214, 0.0207, 0.0211],\n",
       "        ...,\n",
       "        [0.7353, 0.1507, 0.0346,  ..., 0.0215, 0.0207, 0.0211],\n",
       "        [0.7348, 0.1510, 0.0348,  ..., 0.0216, 0.0209, 0.0213],\n",
       "        [0.7349, 0.1513, 0.0348,  ..., 0.0216, 0.0209, 0.0213]],\n",
       "       grad_fn=<SigmoidBackward0>)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(next(iter(each_game.values()))[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [1., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [1., 0., 0.,  ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [1., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 1., 0.,  ..., 0., 0., 0.],\n",
       "        [1., 0., 0.,  ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(each_game.values()))[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
